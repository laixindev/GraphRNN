{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import networkx as nx  \n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "import numpy as np\n",
    "import numpy\n",
    "import sys\n",
    "import json\n",
    "import pdb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from read_graph import read_graphs_in_networkx,save_graphs_nx\n",
    "from utils import calculate_M,graphs_db,encode_M_matrix,decode_M_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs loaded  24000\n"
     ]
    }
   ],
   "source": [
    "data_file = \"../data/citation.txt\"\n",
    "(graphs,node_dict,edge_dict,node_label_freq_dict,edge_label_freq_dict) = read_graphs_in_networkx(data_file,True,100000)\n",
    "print(\"Number of graphs loaded \" , len(graphs))\n",
    "\n",
    "(train_graphs,val_graphs) = train_test_split(graphs,test_size=0.1, random_state=42)\n",
    "(train_graphs,test_graphs) = train_test_split(train_graphs,test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"./models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[1. 1. 1. 1. 1. 1. 1. 1.] [1. 1.]\n",
      "{1: 2995752} {1: 404621, 2: 355468, 3: 210387, 4: 550032, 5: 241411, 6: 581825, 7: 144499}\n"
     ]
    }
   ],
   "source": [
    "len_node_labels = len(node_dict)+1\n",
    "len_edge_labels = len(edge_dict)+1\n",
    "weight_vector_node_label = np.ones(len_node_labels)\n",
    "weight_vector_edge_label = np.ones(len_edge_labels)\n",
    "\n",
    "node_max_value= max(node_label_freq_dict.items(), key=operator.itemgetter(1))[1]\n",
    "\n",
    "\n",
    "for label, count in node_label_freq_dict.items():\n",
    "    weight_vector_node_label[label] = min(1,node_max_value /count)\n",
    "\n",
    "weight_vector_node_label[0]=1\n",
    "edge_count_max_value= max(edge_label_freq_dict.items(), key=operator.itemgetter(1))[1]\n",
    "most_frequent_edge_label = max(edge_label_freq_dict.items(), key=operator.itemgetter(1))[0]\n",
    "print(most_frequent_edge_label)\n",
    "\n",
    "for label, count in edge_label_freq_dict.items():\n",
    "    weight_vector_edge_label[label] = min(1,edge_count_max_value /count)\n",
    "weight_vector_edge_label[0] = 1\n",
    "if len_edge_labels_labels == 2:\n",
    "    weight_vector_edge_label[1] = 4\n",
    "print(weight_vector_node_label,weight_vector_edge_label)\n",
    "print(edge_label_freq_dict,node_label_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs, max nodes, min nodes , max edges , min edges , max prev nodes 19440 241 26 295 50 61\n",
      "124.823 51.20948809546918 103.67679166666667 40.78932247586285\n"
     ]
    }
   ],
   "source": [
    "max_num_edges = max([graph.number_of_edges() for graph in graphs])\n",
    "min_num_edges = min([graph.number_of_edges() for graph in graphs])\n",
    "max_num_nodes = max([graph.number_of_nodes() for graph in graphs])\n",
    "min_num_nodes = min([graph.number_of_nodes() for graph in graphs])\n",
    "M = int(calculate_M(graphs,len(graphs)))\n",
    "print(\"Number of training graphs, max nodes, min nodes , max edges , min edges , max prev nodes\", len(train_graphs), max_num_nodes, min_num_nodes, max_num_edges, min_num_edges,M)\n",
    "mean_number_of_edges = np.mean([graph.number_of_edges() for graph in graphs])\n",
    "std_number_of_edges = np.std([graph.number_of_edges() for graph in graphs])\n",
    "mean_number_of_nodes = np.mean([graph.number_of_nodes() for graph in graphs])\n",
    "std_number_of_nodes = np.std([graph.number_of_nodes() for graph in graphs])\n",
    "print(mean_number_of_edges,std_number_of_edges,mean_number_of_nodes,std_number_of_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_ndx, sample in enumerate(graph_loader):\n",
    "    print(batch_ndx)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training code ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "graph_db = graphs_db(train_graphs,max_num_nodes,M)\n",
    "(a, b,c,d) = graph_db.__getitem__(10)\n",
    "graph_loader = DataLoader(graph_db,batch_size= 32,num_workers=2,shuffle=True)\n",
    "\n",
    "\n",
    "class CUSTOM_RNN_NODE(torch.nn.Module):\n",
    "    def __init__(self, input_size, embedding_size=64, hidden_size=32,output_size =None,number_layers=4,name=\"\",len_unique_node_labels=None,len_unique_edge_labels=None):\n",
    "        super(CUSTOM_RNN_NODE, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.number_layers = number_layers\n",
    "        self.name = name\n",
    "        self.len_unique_node_labels = len_unique_node_labels\n",
    "        self.len_unique_edge_labels = len_unique_edge_labels\n",
    "        \n",
    "        self.sequence_embedding_size = embedding_size*input_size + embedding_size*4\n",
    "        self.input = nn.Embedding(self.len_unique_edge_labels, embedding_size)\n",
    "        self.input2 = nn.Embedding(self.len_unique_node_labels, embedding_size*4)\n",
    "        self.rnn = nn.GRU(input_size=self.sequence_embedding_size,hidden_size = self.hidden_size,\n",
    "                                num_layers=self.number_layers,bias=True,batch_first=True,dropout=0)\n",
    "        self.hidden_n = None\n",
    "        #self.out = nn.Sequential(nn.Linear(self.hidden_size,self.embedding_size),nn.ReLU(),nn.Linear(self.embedding_size,self.output_size))\n",
    "        self.out = nn.Sequential(nn.Linear(self.hidden_size,self.sequence_embedding_size),nn.ReLU(),nn.Linear(self.sequence_embedding_size,self.output_size))\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        ###MLP for loss\n",
    "        self.Linear = nn.Sequential(nn.ReLU(),nn.Linear(self.output_size,self.len_unique_node_labels))\n",
    "    def forward(self,input,x_node_label, seq_lengths = None,is_packed=True,is_MLP=False):\n",
    "        \n",
    "        input = self.input(input)\n",
    "        input = self.relu(input)\n",
    "        input = input.reshape(input.shape[0],input.shape[1],-1)\n",
    "        input2 = self.input2(x_node_label)\n",
    "        input_concat =torch.cat((input, input2), 2)\n",
    "        if is_packed:\n",
    "            input_concat = pack_padded_sequence(input_concat,seq_lengths,batch_first=True,enforce_sorted=False)\n",
    "        output,self.hidden_n = self.rnn(input_concat,self.hidden_n)\n",
    "        \n",
    "        if is_packed:\n",
    "            output = pad_packed_sequence(output,batch_first=True)[0]\n",
    "        output = self.out(output)\n",
    "        if not is_MLP:\n",
    "            return output\n",
    "        \n",
    "        mlp_output= self.Linear(output)\n",
    "        return output,mlp_output\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        return Variable(torch.zeros(self.number_layers, batch_size, self.hidden_size))\n",
    "    \n",
    "class CUSTOM_RNN_EDGE(torch.nn.Module):\n",
    "    def __init__(self, input_size, embedding_size=64, hidden_size=32,output_size =None,number_layers=4,name=\"\",len_unique_edge_labels=None):\n",
    "        super(CUSTOM_RNN_EDGE, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.number_layers = number_layers\n",
    "        self.name = name\n",
    "        self.len_unique_edge_labels = len_unique_edge_labels\n",
    "        \n",
    "        self.embedding= nn.Embedding(self.len_unique_edge_labels,embedding_size)\n",
    "        self.linear = nn.Linear(self.input_size,self.embedding_size)\n",
    "        self.rnn = nn.GRU(input_size=self.embedding_size,hidden_size = self.hidden_size,\n",
    "                                num_layers=self.number_layers,bias=True,batch_first=True,dropout=0)\n",
    "        self.hidden_n = None\n",
    "        self.out = nn.Sequential(nn.Linear(self.hidden_size,self.embedding_size),nn.ReLU(),nn.Linear(self.embedding_size,self.output_size))\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.Linear_mlp = nn.Sequential(nn.ReLU(),nn.Linear(self.output_size,self.len_unique_edge_labels))\n",
    "    def forward(self,input, seq_lengths = None,is_mlp=False):\n",
    "        #print(\"doing forward loop for rnn ,\" ,self.name)\n",
    "        input = self.embedding(input)\n",
    "        input = self.relu(input)\n",
    "        input = input.reshape(input.size(0),input.size(1),-1)\n",
    "        output,self.hidden_n = self.rnn(input,self.hidden_n)\n",
    "        output = self.out(output)\n",
    "        \n",
    "        #if not is_mlp:\n",
    "        return output\n",
    "#         output_mlp = self.Linear_mlp(output)\n",
    "#         return output,output_mlp\n",
    "\n",
    "        \n",
    "### Define two RNNs 1 for graph level and 2nd for edge level \n",
    "# hidden_size_node_rnn = 128\n",
    "# hidden_size_edge_rnn = 32\n",
    "# embedding_size_node_rnn = 64\n",
    "# embedding_size_edge_rnn = 16\n",
    "hidden_size_node_rnn = 128\n",
    "hidden_size_edge_rnn = 64\n",
    "embedding_size_node_rnn = 64\n",
    "embedding_size_edge_rnn = 32\n",
    "num_layers = 4 # old - 4\n",
    "\n",
    "node_rnn = CUSTOM_RNN_NODE(input_size=M, embedding_size=embedding_size_node_rnn,\n",
    "                hidden_size=hidden_size_node_rnn, number_layers=num_layers,output_size=hidden_size_edge_rnn,\n",
    "            name=\"node\",len_unique_node_labels=len_node_labels,len_unique_edge_labels=len_edge_labels)\n",
    "edge_rnn = CUSTOM_RNN_EDGE(input_size=1, embedding_size=embedding_size_edge_rnn,\n",
    "                   hidden_size=hidden_size_edge_rnn, number_layers=num_layers, output_size=len_edge_labels,\n",
    "                    name=\"edge\",len_unique_edge_labels=len_edge_labels)\n",
    "\n",
    "lr = 0.0001\n",
    "optimizer_node = optim.Adam(list(node_rnn.parameters()), lr=lr)\n",
    "optimizer_edge = optim.Adam(list(edge_rnn.parameters()),lr=lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####Epoch####  0\n",
      "0 tensor(0.8711, grad_fn=<AddBackward0>) tensor(0.0749, grad_fn=<NllLossBackward>) tensor(0.7962, grad_fn=<NllLossBackward>)\n",
      "####Epoch####  1\n",
      "0 tensor(0.8450, grad_fn=<AddBackward0>) tensor(0.0762, grad_fn=<NllLossBackward>) tensor(0.7688, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 1\n",
    "for epoch in range(0,epochs):\n",
    "    print(\"####Epoch#### \", epoch)\n",
    "    node_rnn.train()\n",
    "    edge_rnn.train()\n",
    "    for ndx, data in enumerate(graph_loader):\n",
    "        node_rnn.zero_grad()\n",
    "        edge_rnn.zero_grad()\n",
    "        \n",
    "        max_seq_len = max(data[2])\n",
    "        node_labels = data[3].long()\n",
    "        node_rnn.hidden_n = node_rnn.init_hidden(batch_size=list(data[0].size())[0]) \n",
    "        x = data[0].float()[:,0:max_seq_len,:].long()\n",
    "        x[:,0] = x[:,0]*most_frequent_edge_label\n",
    "        y = data[1].float()[:,0:max_seq_len,:].long()\n",
    "        \n",
    "        y_node_labels = node_labels.data.clone()\n",
    "        node_labels = node_labels[:,0:max_seq_len]\n",
    "        y_node_labels[:,0:-1] = y_node_labels[:,1:]\n",
    "        y_node_labels[:,-1] = 0\n",
    "        y_node_labels = y_node_labels[:,0:max_seq_len]\n",
    "        h,h_mlp = node_rnn(x,node_labels, seq_lengths=data[2],is_MLP=True)\n",
    "        h_ce = h_mlp.view(-1,h_mlp.size(2))\n",
    "        h = pack_padded_sequence(h,data[2],batch_first=True,enforce_sorted=False).data\n",
    "        \n",
    "        #criterion = F.cross_entropy(weight=torch.FloatTensor(weight_vector_node_label))\n",
    "        #criterion = nn.CrossEntropyLoss()\n",
    "        #y_node_labels = pack_padded_sequence(y_node_labels,data[2],batch_first=True,enforce_sorted=False).data\n",
    "        y_node_labels = y_node_labels.reshape(-1)\n",
    "        loss_node_label = F.cross_entropy(input=h_ce,target=y_node_labels,weight=torch.Tensor(weight_vector_node_label) ) #weight=torch.Tensor(weight_vector_node_label)\n",
    "        #print(max_seq_len,loss_node_label)\n",
    "        \n",
    "        \n",
    "  \n",
    "#         ## initialized edge rnn with node rnn hiddent state\n",
    "        h_edge_tmp = torch.zeros(num_layers-1, h.size(0), h.size(1))\n",
    "        edge_rnn.hidden_n = torch.cat((h.view(1,h.size(0),h.size(1)),h_edge_tmp),dim=0)\n",
    "        y_packed = pack_padded_sequence(y,data[2],batch_first=True,enforce_sorted=False).data\n",
    "        edge_rnn_y = y_packed.view(y_packed.size(0),y_packed.size(1),1)\n",
    "        edge_rnn_x = torch.cat((torch.ones(edge_rnn_y.size(0),1,1).long()*most_frequent_edge_label,edge_rnn_y[:,0:-1,0:1]),dim=1)\n",
    "        \n",
    "        \n",
    "        edge_rnn_y_pred = edge_rnn(edge_rnn_x)\n",
    "        #print(edge_rnn_y_pred.size())\n",
    "        \n",
    "        \n",
    "#         output_y_len = []\n",
    "#         output_y_len_bin = np.bincount(np.array(data[2]))\n",
    "#         for i in range(len(output_y_len_bin)-1,0,-1):\n",
    "#             count_temp = np.sum(output_y_len_bin[i:]) # count how many y_len is above i\n",
    "#             output_y_len.extend([min(i,y.size(2))]*count_temp)\n",
    "#         print(output_y_len)\n",
    "        #edge_rnn_y_pred = pack_padded_sequence(edge_rnn_y_pred,data[2],enforce_sorted=False).data\n",
    "        #edge_rnn_y = pack_padded_sequence(edge_rnn_y,data[2],enforce_sorted=False).data.reshape(-1)\n",
    "        #print(edge_rnn_y_pred.size(),edge_rnn_y.size())\n",
    "        \n",
    "        #edge_rnn_y_pred = F.sigmoid(edge_rnn_y_pred)\n",
    "        edge_rnn_y = edge_rnn_y.reshape(-1)\n",
    "        edge_rnn_y_pred = edge_rnn_y_pred.view(-1,edge_rnn_y_pred.size(2))\n",
    "        loss_edge_label = F.cross_entropy(edge_rnn_y_pred, edge_rnn_y,weight=torch.Tensor(weight_vector_edge_label))\n",
    "        #print(loss_edge_label)\n",
    "        \n",
    "        total_loss = loss_edge_label+loss_node_label\n",
    "        total_loss.backward()\n",
    "        optimizer_edge.step()\n",
    "        optimizer_node.step()\n",
    "        print(ndx,total_loss,loss_edge_label,loss_node_label)\n",
    "\n",
    "        #print(h.size())\n",
    "\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     0],\n",
       "        [    61],\n",
       "        [   122],\n",
       "        ...,\n",
       "        [212587],\n",
       "        [212647],\n",
       "        [212708]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_rnn_y.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9609, -3.5471],\n",
       "        [ 0.5574, -1.2003],\n",
       "        [ 1.0014, -1.9575],\n",
       "        [ 1.2261, -2.3495]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_rnn_y_pred[60:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = data_file.split(\"/\")[-1].split(\".\")[0]\n",
    "print(\"saving to, \" ,model_save_path+epoch)\n",
    "fname = model_save_path + 'node_' + str(epoch) + '.dat'\n",
    "print(\"saving node rnn to,\",fname)\n",
    "torch.save(node_rnn.state_dict(), fname)\n",
    "fname = model_save_path +  'edge_' + str(epoch) + '.dat'\n",
    "print(\"saving edge rnn to, \",fname)\n",
    "torch.save(edge_rnn.state_dict(), fname)\n",
    "model_parameters={}\n",
    "model_parameters['node_label_dict'] = node_dict\n",
    "model_parameters['edge_label_dict'] = edge_dict\n",
    "model_parameters['node_weight_label_dict'] = weight_vector_node_label\n",
    "model_parameters['edge_weight_label_dict'] = weight_vector_edge_label\n",
    "model_parameters['node_label_freq_dict'] = node_label_freq_dict\n",
    "model_parameters['edge_label_freq_dict'] = edge_label_freq_dict\n",
    "model_parameters['mean_std_nodes'] = (mean_number_of_edges,std_number_of_edges)\n",
    "model_parameters['mean_std_edges'] = (mean_number_of_nodes,std_number_of_nodes)\n",
    "model_parameters['max_num_nodes'] =max_num_nodes\n",
    "model_parameters['min_num_nodes'] = min_num_nodes\n",
    "model_parameters['len_edges'] = len_edge_labels\n",
    "model_parameters['len_nodes']= len_node_labels\n",
    "model_parameters['hidden_size_node_rnn'] = hidden_size_node_rnn\n",
    "model_parameters['hidden_size_edge_rnn'] = hidden_size_edge_rnn\n",
    "model_parameters['embedding_size_node_rnn'] = embedding_size_node_rnn\n",
    "model_parameters['embedding_size_edge_rnn'] = embedding_size_edge_rnn\n",
    "model_parameters['num_layers'] = num_layers\n",
    "model_parameters['M'] = M\n",
    "model_parameters['most_frequent_edge_label'] = most_frequent_edge_label\n",
    "fname = model_save_path + \"parameters_\" + str(epoch) + '.pkl'\n",
    "print(\"saving parameters to,\" , fname)\n",
    "pickle.dump(model_parameters,open(fname,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge_rnn_y[0:20]\n",
    "# edge_rnn_y_pred[0:20]\n",
    "# print(h_ce.reshape(32,-1,12)[0])\n",
    "# print(y_node_labels.reshape(32,-1)[0])\n",
    "#node_labels[0]\n",
    "#node_labels.reshape(32,-1)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_random_label(label_freq_dict):\n",
    "    freq_dict = [(key,value) for key,value in label_freq_dict.items() ]\n",
    "    freq_dict =sorted(freq_dict,key=lambda val:val[1],reverse=True)\n",
    "    return random.choice((freq_dict[0][0],freq_dict[1][0]))\n",
    "# def sample_multi(y,num_of_samples=1):\n",
    "#     #print(y)\n",
    "#     y = F.softmax(y,dim=2)\n",
    "#     sampled_y = torch.mode(torch.multinomial(y.view(y.size(0),y.size(2)),num_samples=num_of_samples,replacement=True))[0]\n",
    "#     #print(sampled_y)\n",
    "#     return sampled_y.reshape(-1,1)\n",
    "\n",
    "\n",
    "\n",
    "def sample_multi(y,num_of_samples=1):\n",
    "    #print(y)\n",
    "    y = F.softmax(y,dim=2)\n",
    "    #argmax = torch.argmax(y, dim =2)\n",
    "    torch_multi = torch.multinomial(y.view(y.size(0),y.size(2)),num_of_samples,replacement=True)\n",
    "    sampled_y = torch.mode(torch_multi, dim =1)\n",
    "    #return argmax.reshape(-1,1)\n",
    "    return sampled_y.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n"
     ]
    }
   ],
   "source": [
    "num_graphs_to_be_generated = 1000\n",
    "node_rnn.hidden_n = node_rnn.init_hidden(num_graphs_to_be_generated)\n",
    "node_rnn.eval()\n",
    "edge_rnn.eval()\n",
    "generated_graphs =torch.zeros(num_graphs_to_be_generated, max_num_nodes-1, M)\n",
    "generated_graphs_labels = torch.zeros(num_graphs_to_be_generated,max_num_nodes-1,1)\n",
    "node_x = torch.ones(num_graphs_to_be_generated,1,M).long()*most_frequent_edge_label\n",
    "node_x_label = torch.ones(num_graphs_to_be_generated,1).long()\n",
    "for i in range(0,num_graphs_to_be_generated):\n",
    "    node_x_label[i,0]=pick_random_label(node_label_freq_dict)\n",
    "    #node_x_label[i,0] = 2\n",
    "node_x_label_1st_node = node_x_label\n",
    "\n",
    "print(\"generating\")\n",
    "for i in range(0,max_num_nodes-1):\n",
    "    print(i)\n",
    "    h,h_mlp = node_rnn(node_x,node_x_label,None,is_packed=False,is_MLP=True)\n",
    "    node_label_sampled = sample_multi(h_mlp,num_of_samples=1)\n",
    "    h_edge_tmp = torch.zeros(num_layers-1, h.size(0), h.size(2))\n",
    "    edge_rnn.hidden_n = torch.cat((h.permute(1,0,2),h_edge_tmp),dim=0)\n",
    "    edge_x = torch.ones(num_graphs_to_be_generated,1,1).long()*most_frequent_edge_label\n",
    "    \n",
    "    \n",
    "    node_x = torch.zeros(num_graphs_to_be_generated,1,M).long()\n",
    "    node_x_label = node_label_sampled.long()\n",
    "    \n",
    "    \n",
    "    for j in range(min(M,i+1)):\n",
    "        edge_rnn_y_pred = edge_rnn(edge_x)\n",
    "        edge_rnn_y_pred_sampled = sample_multi(edge_rnn_y_pred,num_of_samples=1)\n",
    "        #print(edge_rnn_y_pred_sampled.size(),node_x[:,:,j:j+1].size())\n",
    "        node_x[:,:,j:j+1] = edge_rnn_y_pred_sampled.view(edge_rnn_y_pred_sampled.size(0),\n",
    "                                                         edge_rnn_y_pred_sampled.size(1),1)\n",
    "        edge_x = edge_rnn_y_pred_sampled.long()\n",
    "    \n",
    "    \n",
    "    #print(node_label_sampled.size(),generated_graphs_labels[:,i+1,:].size())\n",
    "    generated_graphs_labels[:,i] = node_label_sampled\n",
    "    generated_graphs[:, i:i + 1, :] = node_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_label_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_label_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, 7220), (1.0, 21891), (2.0, 23091), (3.0, 13391), (4.0, 81262), (5.0, 16051), (6.0, 70222), (7.0, 6872)]\n"
     ]
    }
   ],
   "source": [
    "a,b = np.unique(generated_graphs_labels,return_counts=True)\n",
    "print([i for i in zip(a,b)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_graphs[3,:].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_graphs_labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs, max nodes, min nodes , max edges , min edges , max prev nodes 4 1 3 0 61\n"
     ]
    }
   ],
   "source": [
    "# # remove all zeros rows and columns\n",
    "# adj = adj[~np.all(adj == 0, axis=1)]\n",
    "# adj = adj[:, ~np.all(adj == 0, axis=0)]\n",
    "# adj = np.asmatrix(adj)\n",
    "# G = nx.from_numpy_matrix(adj)\n",
    "# return G\n",
    "\n",
    "def cut_graph(g,labels):\n",
    "    tp = np.where(~g.any(axis=1))[0]\n",
    "    if tp.shape[0] >0:\n",
    "        g = g[0:tp[0],:]\n",
    "        labels = labels[0:tp[0],:]\n",
    "    ct = 0\n",
    "    index = None\n",
    "    ct = 0\n",
    "    labels_list = []\n",
    "    for i in labels:\n",
    "        if i[0] == 0:### terminal node\n",
    "            index = ct\n",
    "            break\n",
    "        labels_list.append(i[0])\n",
    "        ct += 1\n",
    "    return (g[0:ct,:],labels_list)\n",
    "predicted_graphs = []\n",
    "predicted_graphs_x = []\n",
    "predicted_graphs_x_labels = []\n",
    "for i in range(num_graphs_to_be_generated):\n",
    "    pred_graph,pred_labels = cut_graph(generated_graphs[i].numpy(),generated_graphs_labels[i].numpy())\n",
    "    #pred_labels = [item[0] for item in generated_graphs_labels[i]]\n",
    "    #pred_graph = generated_graphs[i].numpy()\n",
    "    predicted_graphs.append(decode_M_matrix(pred_graph,M))\n",
    "    predicted_graphs_x.append(nx.from_numpy_matrix(predicted_graphs[i]))\n",
    "    start_label = node_x_label_1st_node[i,0].tolist()\n",
    "    pred_labels.insert(0,start_label)\n",
    "    predicted_graphs_x_labels.append(pred_labels)\n",
    "max_num_edges_test = max([graph.number_of_edges() for graph in predicted_graphs_x])\n",
    "min_num_edges_test = min([graph.number_of_edges() for graph in predicted_graphs_x])\n",
    "max_num_nodes_test = max([graph.number_of_nodes() for graph in predicted_graphs_x])\n",
    "min_num_nodes_test = min([graph.number_of_nodes() for graph in predicted_graphs_x])\n",
    "print(\"Number of training graphs, max nodes, min nodes , max edges , min edges , max prev nodes\", max_num_nodes_test, min_num_nodes_test, max_num_edges_test, min_num_edges_test,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15 0.42367440328629724\n",
      "0.15 0.4236744032862972\n"
     ]
    }
   ],
   "source": [
    "tp = [graph.number_of_nodes() for graph in predicted_graphs_x]\n",
    "print(np.mean(tp),np.std(tp))\n",
    "\n",
    "tp_e= [graph.number_of_edges() for graph in predicted_graphs_x]\n",
    "print(np.mean(tp_e),np.std(tp_e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[graph.number_of_nodes() for graph in predicted_graphs_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_graphs_x = [graph for graph in predicted_graphs_x if graph.number_of_edges() > 5 and graph.number_of_edges() < 65]\n",
    "# print(len(predicted_graphs_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_id_to_label = {value:key for key,value in node_dict.items()}\n",
    "edge_id_to_label = {value:key for key,value in edge_dict.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in range(0,len(predicted_graphs_x)):\n",
    "    graph = predicted_graphs_x[id]\n",
    "    for i in range(0,graph.number_of_nodes()):\n",
    "        graph.nodes[i]['node_label'] = int(predicted_graphs_x_labels[id][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_graphs_nx(predicted_graphs_x,\"predicted_graphs_MLP_num_layers_2.txt\",node_id_to_label,edge_id_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_graphs_nx(test_graphs[0:2000],\"true_graphs_MLP_num_layers_2.txt\",node_id_to_label,edge_id_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_graphs_x[10].nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "\n",
    "def draw_graph(g):\n",
    "    groups = set(nx.get_node_attributes(g,'node_label').values())\n",
    "    mapping = dict(zip(sorted(groups),count()))\n",
    "    nodes = g.nodes()\n",
    "    colors = [mapping[g.node[n]['node_label']] for n in nodes]\n",
    "\n",
    "# drawing nodes and edges separately so we can capture collection for colobar\n",
    "    pos = nx.spring_layout(g)\n",
    "    ec = nx.draw_networkx_edges(g, pos, alpha=0.2)\n",
    "    nc = nx.draw_networkx_nodes(g, pos, nodelist=nodes, node_color=colors, \n",
    "                            with_labels=False, node_size=100, cmap=plt.cm.jet)\n",
    "    plt.colorbar(nc)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(graphs[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(predicted_graphs_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## nx.draw(predicted_graphs_x[10],labels=grawith_labels=True, cmap = plt.get_cmap('jet'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(graphs[16], cmap = plt.get_cmap('jet'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
